# ğŸ—£ï¸ Voice Command Recognition on Raspberry Pi Pico W ğŸ™ï¸

A TinyML project for embedded voice command recognition on the Raspberry Pi Pico W.

## ğŸ’¡ Overview

This project explores the implementation of a voice command recognition system on the Raspberry Pi Pico W microcontroller. Leveraging the device's low cost, flexibility, and limited resource capabilities, we've created a streamlined system capable of processing audio signals and performing inference using pre-trained machine learning models. ğŸš€

This project is divided into two main phases:

*   **Phase 1: Proof of Concept** - Demonstrating the end-to-end audio processing pipeline using synthetic audio samples.
*   **Phase 2: Enhanced Realism** - Improving robustness and accuracy by integrating real-world audio data and refined feature extraction techniques.

## âœ¨ Key Features

*   **Embedded Implementation:** Runs directly on the Raspberry Pi Pico W, eliminating the need for cloud processing.
*   **Low Resource Footprint:** Optimized algorithms and model architecture for resource-constrained devices.
*   **Real-time Performance:**  Designed for real-time voice command recognition.
*   **Clear Feedback:** Provides visual and auditory feedback on command recognition success. âœ…
*   **Simulated Environment:** Developed and tested within the Wokwi online simulator for easy experimentation. ğŸ’»

## ğŸ› ï¸ Technologies Used

*   **Raspberry Pi Pico W:** Microcontroller for audio processing and inference. ğŸ“
*   **C:** Programming language for firmware development. âš™ï¸
*   **TensorFlow Lite Micro:**  Framework for deploying optimized machine learning models.
*   **Wokwi:** Online simulator for embedded systems. ğŸŒ
*   **MFCC (Mel-Frequency Cepstral Coefficients):** Feature extraction technique for audio analysis. ğŸ¶
*   **TinyML:** Machine learning on embedded devices. ğŸ§ 

## âš™ï¸ How It Works

The voice command recognition system follows these steps:

1.  **Audio Acquisition:** The system captures audio from a microphone (simulated in the Wokwi environment).
2.  **Feature Extraction:** Mel-Frequency Cepstral Coefficients (MFCCs) are extracted from the audio signal to capture relevant acoustic characteristics.
3.  **Neural Network Inference:** A pre-trained neural network processes the MFCCs and predicts the spoken command.
4.  **Feedback:** The system provides feedback to the user through an LED (visual) and a buzzer (auditory), confirming the recognized command.


## ğŸš€ Getting Started

Unfortunately, this project can't be directly cloned and run without a Raspberry Pi Pico W or Wokwi account. However you can follow the methodology, and adapt the source to your own project.

1.  **Prerequisites:**
    *   Raspberry Pi Pico W
    *   Wokwi Account (optional, for simulation)
    *   Basic knowledge of C programming.
    *   Raspberry Pi Pico C/C++ SDK installed and configured on your development machine.

2.  **Setup:**
    *   (For Wokwi simulation): Create a free account at [wokwi.com](https://wokwi.com/) and clone the source code to use in the environment.

3.  **Build the Firmware:**

4.  **Flash the Firmware:** Connect the Raspberry Pi Pico W to your computer and flash the compiled firmware.

5.  **Test and Experiment:** Speak the pre-defined commands ("go", "stop", "up", "down", "left", "right") and observe the system's feedback.

## ğŸ“‚ Project Structure

â”œâ”€â”€ main.c # Main source code file
â”œâ”€â”€ diagram.json # Wokwi simulator configuration
â”œâ”€â”€ model_data_modelo_simplificado.h # Pre-trained neural network weights
â”œâ”€â”€ test_audio_samples.h # Real audio samples for testing
â””â”€â”€ ...

## ğŸ“ˆ Performance and Results

The initial version of the system achieved a recognition accuracy of **88%** in a controlled environment. The project improved upon this by using prerecorded auido samples and reducing the neural network. However some problems were identified:

*   Overflow on Softmax
*   Prediction Concentrated

These will be addressed in the future for increasing the robustness and precise system.

## ğŸ—ºï¸ Roadmap

*   Address overflows in the Softmax function to improve stability.
*   Implement techniques to improve the model's generalization to a wider range of speakers and environments.
*   Explore data augmentation techniques to increase the robustness of the model.
*   Evaluate and compare different model architectures to optimize performance.
*   Implement noise reduction algorithms to improve recognition accuracy in noisy environments. (Work in progress)

## ğŸ¤ Contributions

Contributions to this project are welcome! Feel free to submit bug reports, feature requests, or pull requests.


## âœï¸ Author

*   Leonardo Maximino Bernardo

## ğŸ”— Useful Links

*   **Wokwi Simulation:** [https://wokwi.com/projects/420983118254131201](https://wokwi.com/projects/420983118254131201)
*   **Second Version**: [https://wokwi.com/projects/420267737999434753](https://wokwi.com/projects/420267737999434753)
*   **Raspberry Pi Pico C/C++ SDK Documentation:** [https://datasheets.raspberrypi.com/picow/pico-2-w-schematic.pdf](https://datasheets.raspberrypi.com/picow/pico-2-w-schematic.pdf)
*   **Mini Speech Commands Dataset:** (if applicable) [https://www.tensorflow.org/tutorials/audio/simple\_audio?hl=pt-br](https://www.tensorflow.org/tutorials/audio/simple_audio?hl=pt-br)

